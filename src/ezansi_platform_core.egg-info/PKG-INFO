Metadata-Version: 2.4
Name: ezansi-platform-core
Version: 0.1.0
Summary: The stable, reusable foundation for the ezAnsi AI edge computing platform
Requires-Python: >=3.11
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: fastapi==0.115.6
Requires-Dist: uvicorn[standard]==0.30.6
Requires-Dist: httpx==0.27.2
Requires-Dist: pydantic==2.10.4
Requires-Dist: pyyaml==6.0.2
Provides-Extra: test
Requires-Dist: pytest==8.0.0; extra == "test"
Requires-Dist: pytest-asyncio==0.23.5; extra == "test"
Requires-Dist: pytest-cov==4.1.0; extra == "test"
Dynamic: license-file

# ezansi-platform-core

The stable, reusable foundation for the ezAnsi AI edge computing platform. This repository contains the core orchestration, discovery, and composition infrastructure that enables independent AI capabilities to work together seamlessly.

## Status

**Phase 1** ✓ Complete - Base Ollama capability deployed and tested
**Phase 2** → In Progress - Building platform orchestration layer
**Phase 3** → Planned - Full composition and orchestration

## Getting Started

- New to the project? Start with [Deployment Guide](docs/deployment-guide.md)
- Want to build on platform-core? See [Phase 2 Architecture](docs/phase-2-architecture/)
- Looking for the LLM capability? See [ezansi-capability-llm-ollama](https://github.com/eZansiEdgeAI/ezansi-capability-llm-ollama)

### Quickstart (runs the gateway)

This repository now includes a minimal runnable gateway (registry + router + validation) intended for early stack demos.

```bash
cd ezansi-platform-core
podman-compose up -d

# Verify
curl -fsS http://localhost:8000/health
curl -fsS http://localhost:8000/registry
```

By default the gateway scans `./capabilities/**/capability.json` (mounted into the container as `/capabilities`).

If capability contracts use `http://localhost:<port>`, the gateway can still reach host-published capability ports via `config/overrides.yaml` (enabled by default via `OVERRIDES_PATH`).

### Managing containers with Podman (start/stop)

`podman-compose up -d` is the easiest way to create the containers the first time (networks, volumes, env wiring).
After they exist, you can start/stop them directly with Podman:

```bash
# Start (existing containers)
podman start \
  ezansi-platform-core \
  chromadb-retrieval-chroma \
  chromadb-retrieval-capability \
  ollama-llm-capability

# Stop
podman stop -t 10 \
  ezansi-platform-core \
  chromadb-retrieval-capability \
  chromadb-retrieval-chroma \
  ollama-llm-capability
```

If you prefer not to rely on container names, you can also target containers created by `podman-compose` using the compose project label:

```bash
# Start everything created by a compose project
podman start $(podman ps -aq --filter label=io.podman.compose.project=ezansi-platform-core)
podman start $(podman ps -aq --filter label=io.podman.compose.project=ezansi-capability-llm-ollama)
podman start $(podman ps -aq --filter label=io.podman.compose.project=ezansi-capability-retrieval-chromadb)
```

Note: `--restart=unless-stopped` helps with unexpected restarts, but for guaranteed “start on boot” use systemd user services.

## Architecture Overview

The platform is built on a **LEGO brick model**:
- **Platform Core** (this repo): Stable foundation, rarely changes
- **Capabilities** (separate repos): Pluggable AI services (LLM, STT, TTS, Vision, etc.)
- **Experience Stacks** (composed): User-facing applications combining multiple capabilities

### Three-Layer Design

```
┌─────────────────────────────────────┐
│  Experience Stacks                  │  Composed applications (voice assistant, etc.)
│  (voice-assistant, chat-ui, etc)    │
└────────────────┬────────────────────┘
                 │
┌────────────────▼────────────────────┐
│  Platform Core (this repo)          │  Orchestration, discovery, routing
│  - Registry                         │
│  - Capability Loader                │
│  - Request Router                   │
│  - Resource Manager                 │
└────────────────┬────────────────────┘
                 │
┌────────────────▼────────────────────┐
│  Capabilities (separate repos)      │  Independent, replaceable modules
│  - ollama (LLM)                     │
│  - whisper (STT)                    │
│  - piper (TTS)                      │
│  - vision (Image processing)        │
└─────────────────────────────────────┘
```

## Capability Contract

All capabilities follow a standardized interface defined in `capability.json`. This enables the platform to:
- Discover what each capability provides
- Validate resource requirements before deployment
- Route requests to the appropriate service
- Compose multiple capabilities into workflows

### Example Capability Contract

```json
{
  "name": "ollama-llm",
  "version": "1.0",
  "provides": ["text-generation"],
  "api": {"endpoint": "http://localhost:11434", "health_check": "/api/tags"},
  "resources": {
    "ram_mb": 6000,
    "cpu_cores": 4
  },

  "target_platforms": ["Raspberry Pi 4/5", "AMD64 (x86-64)"],
  "supported_architectures": ["arm64", "amd64"],
  "target_platform": "Raspberry Pi 4/5"
}
```

See [ezansi-capability-llm-ollama](https://github.com/eZansiEdgeAI/ezansi-capability-llm-ollama) for a complete implementation.

## Current Status

**v0.1.0 (Demo-ready core)**
- ✅ Minimal API gateway (FastAPI)
- ✅ File-based registry discovery (`capability.json` scanning)
- ✅ Type-based request routing (`POST /` with `{type, payload}`)
- ✅ Resource validation for stacks (`POST /validate/stack`)
- ⏳ Background health monitoring + richer routing transforms
- ⏳ Stack composition blueprints + advisor layer (kept outside platform-core; see `ezansi-blueprints`)

This is intentionally minimal. The platform grows only when capabilities need platform features.

## When to Update Platform-Core

Update this repo when:
- Multiple capabilities reveal a common platform need
- Contract specification needs refinement (discovered through real-world capability testing)
- Platform bugs are discovered
- New composition patterns emerge
- Testing reveals platform limitations

**Do NOT update when:**
- Adding new capabilities (they should work with existing platform)
- Building capability-specific features (stay in capability repo)
- Just keeping pace with capability development

## Development Guidelines

### Adding a New Capability

1. Create separate repo: `ezansi-capability-<name>-<service>`
2. Implement `capability.json` contract following the spec
3. Containerize with Podman
4. Include deployment docs, health checks, test suite
5. Platform auto-discovers it via registry

### Updating Platform Features

1. Only if **multiple capabilities need it**
2. Design against the capability contract, not specific implementations
3. Keep breaking changes minimal
4. Update all existing capabilities' documentation

### Testing

The platform includes a comprehensive end-to-end test suite covering:

- **Core Platform Tests**: Health checks, discovery, routing, validation
- **Role-Based Scenarios**: Developer, user, admin, and integration workflows
- **Hardware-Specific Tests**: Raspberry Pi 5 deployment validation
- **Integration Tests**: Multi-capability composition

**Quick Start:**
```bash
# Install test dependencies
pip install -r requirements-test.txt

# Run all tests
pytest -v

# Run specific categories
pytest -v -m e2e           # Core platform tests
pytest -v -m scenario      # Role-based scenarios
pytest -v -m hardware      # Pi5 hardware tests
```

**Continuous Integration**: Tests run automatically on all PRs via GitHub Actions.

**For detailed testing documentation, see [Test Guide](tests/TEST_GUIDE.md)**

## Quick Links

- [ezansi-capability-llm-ollama](https://github.com/eZansiEdgeAI/ezansi-capability-llm-ollama) - First capability (reference implementation)
- [Capability Contract Specification](https://github.com/eZansiEdgeAI/ezansi-capability-llm-ollama/blob/main/docs/capability-contract-spec.md)
- [Architecture Deep Dive](https://github.com/eZansiEdgeAI/ezansi-capability-llm-ollama/blob/main/docs/architecture.md)
- [Deployment & Portability Guide](docs/deployment-guide.md)

## License

Same as parent project.
